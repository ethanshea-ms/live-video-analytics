{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "# Create a Local Docker Image\n",
        "In this section, we will create an IoT Edge module, a Docker container image with an HTTP web server that has a scoring REST endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../../common')\n",
        "from env_variables import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Web Application & Inference Server for Our ML Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. To change the inference model that will be used in this sample, change the variable `IS_MODEL_NAME` based on your preferred model. Note that the model must be downloaded, as instructed in the [previous section](create_openvino_inference_engine.ipynb). The default model is         \"person-vehicle-bike-detection-crossroad-1016\".\n",
        "\n",
        "2. Our model currently only accepts JPEG images. Yet, OpenCV supports many more [image formats](https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html), including PNG, TIFF, etc. If you wish to use other image formats, change the following lines of code:\n",
        "\n",
        "    ```\n",
        "    if request.headers['Content-Type'] != 'image/jpeg':\n",
        "        logger.info(\"[AI EXT] Non JPEG content sent. Exiting the scoring event...\")\n",
        "    ```\n",
        "\n",
        "3. The variable assignment of `IS_TARGET_DEVICE` indicates what type of hardware acceleration you would like to use on your IoT Edge device. You can choose from the following choices:\n",
        "    * \"CPU\" for Intel CPU acceleration  \n",
        "    * \"MYRIAD\" for Intel VPU acceleration\n",
        "    * \"GPU\" for Intel GPU acceleration\n",
        "    * \"FPGA\" for Intel FPGA acceleration\n",
        "\n",
        "    Change the variable `IS_TARGET_DEVICE` as needed; the default is \"CPU\". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/app.py\n",
        "import os\n",
        "from flask import Flask, request, Response\n",
        "import cv2\n",
        "import logging\n",
        "import json\n",
        "import io\n",
        "from score import AnalyticsAPI\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import threading\n",
        "from azure.iot.device import IoTHubModuleClient, MethodResponse\n",
        "\n",
        "# Initial settings of AI model\n",
        "IS_MODEL_NAME = \"person-vehicle-bike-detection-crossroad-1016\" # see AnalyticsAPI class for full list of models and other possibilities\n",
        "IS_TARGET_DEVICE = \"CPU\"\n",
        "IS_MODEL_PRECISION = \"FP32\"\n",
        "IS_PROBABILITY_THRESHOLD = 0.5\n",
        "IS_DEBUG_PATH = \"/debug_path/\" # Same value used in Manifest file. If you change this file, also change it in the deployment manifest.\n",
        "\n",
        "app = Flask(__name__)\n",
        "analyticsAPI = AnalyticsAPI(    modelName=IS_MODEL_NAME, \n",
        "                                modelPrecision=IS_MODEL_PRECISION, \n",
        "                                targetDev=IS_TARGET_DEVICE, \n",
        "                                probThreshold=IS_PROBABILITY_THRESHOLD)\n",
        "\n",
        "# Wait some time to have the API - HW accelerators initialized\n",
        "time.sleep(2)\n",
        "logger = analyticsAPI.logger\n",
        "\n",
        "\n",
        "# # Debug flag for writing out processed frames with bounding boxes etc. marked on them.\n",
        "# # if set, debug data will be written in to \"/debug_path\" folder, which is bind to host PC's local drive.\n",
        "# # Host PC local drive is specified in the Manifest file, and default value is host PC's /tmp/IS_DEBUG folder.\n",
        "# # /tmp already exist in host pc but you should manually create and give access credentails to /tmp/IS_DEBUG (or any directory you want...)\n",
        "# #\n",
        "# # 0 : no debug output\n",
        "# # 1 : output frames only if object detected\n",
        "# # 2 : output all frames received\n",
        "# #\n",
        "# IS_DEBUG = 2\n",
        "\n",
        "# # IoT Hub cloud to device method handles\n",
        "# # https://docs.microsoft.com/en-us/python/api/azure-iot-device/azure.iot.device?view=azure-python (see IoTHubModuleClient)\n",
        "# #\n",
        "# def directMethodListener(client):\n",
        "#     global analyticsAPI\n",
        "#     global IS_DEBUG\n",
        "#     while True:\n",
        "#         try:\n",
        "#             methodRequest = client.receive_method_request()  # blocking call\n",
        "#             logger.info(\"[AI EXT] Method invoked: {0} - {1}\".format(methodRequest.name, methodRequest.payload))\n",
        "\n",
        "#             if methodRequest.name == \"SetProbabilityThreshold\":\n",
        "#                 analyticsAPI.setProbabilityThreshold(float(methodRequest.payload))\n",
        "#                 callResult = str(analyticsAPI.getProbabilityThreshold())\n",
        "\n",
        "#             elif methodRequest.name == \"SetModel\":\n",
        "#                 analyticsAPI.setModelName(str(methodRequest.payload))\n",
        "#                 callResult = analyticsAPI.getModelName()\n",
        "\n",
        "#             elif methodRequest.name == \"SetModelPrecision\":\n",
        "#                 analyticsAPI.setModelPrecision(str(methodRequest.payload))\n",
        "#                 callResult = analyticsAPI.getModelPrecision()\n",
        "\n",
        "#             elif methodRequest.name == \"SetTargetDevice\":\n",
        "#                 analyticsAPI.setTargetDevice(str(methodRequest.payload))\n",
        "#                 callResult = analyticsAPI.getTargetDevice()\n",
        "\n",
        "#             elif methodRequest.name == \"SetDebug\":\n",
        "#                 IS_DEBUG = int(methodRequest.payload)\n",
        "#                 callResult = str(IS_DEBUG)\n",
        "\n",
        "#             else:\n",
        "#                 continue\n",
        "\n",
        "#             responsePayload = {\"Success\": \"{0} - {1}\".format(methodRequest.name, callResult)}\n",
        "#             methodResponse = MethodResponse(methodRequest.request_id, 200, payload=responsePayload)\n",
        "#             client.send_method_response(methodResponse)\n",
        "\n",
        "#         except Exception as e:\n",
        "#             responsePayload = {\"Exception\": \"{0} - {1}\".format(methodRequest.name, str(e))}\n",
        "#             methodResponse = MethodResponse(methodRequest.request_id, 400, payload=responsePayload)\n",
        "#             client.send_method_response(methodResponse)\n",
        "\n",
        "# #\n",
        "# # Initialize IoTHubDeviceClient to listen method calls coming from cloud.\n",
        "# #\n",
        "# iotHubModuleClient = IoTHubModuleClient.create_from_edge_environment(websockets=True)\n",
        "# iotHubModuleClient.connect()\n",
        "\n",
        "# # Start IoTHub event callback threads\n",
        "# threadDirectMethodListener = threading.Thread(target=directMethodListener, args=(iotHubModuleClient,))\n",
        "# threadDirectMethodListener.daemon = True\n",
        "# threadDirectMethodListener.start()\n",
        "\n",
        "# TODO: handle twin updates...\n",
        "\n",
        "@app.route(\"/score\", methods = ['POST'])\n",
        "def scoreRRS():\n",
        "    global analyticsAPI\n",
        "    global logger\n",
        "    global IS_DEBUG\n",
        "    \n",
        "    # uncomment for extended debug log\n",
        "    # logger.info(\"[AI EXT] Received scoring request. Header: {0}\".format(json.dumps(dict(request.headers))))\n",
        "    # logger.info(\"[AI EXT] Received scoring request\")\n",
        "\n",
        "    try:\n",
        "        # We accept only \"image/jpeg\" in this sample, but you can change the line below for other formats. \n",
        "        if request.headers['Content-Type'] != 'image/jpeg':\n",
        "            logger.info(\"[AI EXT] Non JPEG content sent. Exiting the scoring event...\")\n",
        "            #return Response(json.dumps({}), status= 415, mimetype ='application/json')\n",
        "            return Response(status= 500) #Internal Server error - not expected\n",
        "\n",
        "        # Before going forward, return not ready, if it is the case...\n",
        "        if not analyticsAPI.initialized:\n",
        "            logger.info(\"[AI EXT] {}\".format(analyticsAPI.INF_STAT_NOT_READY))\n",
        "            return  Response(status= 500) #Internal Server error - not expected\n",
        "            #return  Response(json.dumps({\"status\": analyticsAPI.INF_STAT_NOT_READY}), status= 200, mimetype ='application/json')  \n",
        "\n",
        "        # get request as byte stream\n",
        "        reqBody = request.get_data(False)\n",
        "\n",
        "        # convert from byte stream\n",
        "        inMemFile = io.BytesIO(reqBody)\n",
        "\n",
        "        # load a sample image\n",
        "        inMemFile.seek(0)\n",
        "        fileBytes = np.asarray(bytearray(inMemFile.read()), dtype=np.uint8)\n",
        "        cvImage = cv2.imdecode(fileBytes, cv2.IMREAD_COLOR)\n",
        "\n",
        "        # call scoring function\n",
        "        result = analyticsAPI.score(cvImage)            \n",
        "\n",
        "        # color = (255, 0, 0) # in BGR order\n",
        "\n",
        "        # if IS_DEBUG > 0:\n",
        "        #     jsonResult = json.loads(result)\n",
        "        #     statusCode = int(jsonResult[\"status\"])\n",
        "        #     objectCount = 0\n",
        "            \n",
        "            # if statusCode == 0:\n",
        "            #     objectCount = int(jsonResult[\"object_count\"])\n",
        "            #     if objectCount > 0:\n",
        "            #         jsonResult = jsonResult[\"result\"]\n",
        "            #         for k in jsonResult:\n",
        "            #             detRes = jsonResult[k]\n",
        "            #             xmin = int(detRes[\"xmin\"])\n",
        "            #             ymin = int(detRes[\"ymin\"])\n",
        "            #             xmax = int(detRes[\"xmax\"])\n",
        "            #             ymax = int(detRes[\"ymax\"])\n",
        "            #             classId = str(detRes[\"label\"])\n",
        "            #             confidence = str(detRes[\"confidence\"])\n",
        "\n",
        "            #             cv2.rectangle(cvImage, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "            #             cv2.putText(cvImage, classId + \" - \" + confidence, (xmin, ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 1, color, 1)\n",
        "\n",
        "            # if (IS_DEBUG > 1) or (objectCount > 0):\n",
        "            #     camID = request.args.get('cam', default = 0, type = int)\n",
        "            #     outPath = os.path.join(IS_DEBUG_PATH, \"cam{0:02d}\".format(camID))\n",
        "            #     os.makedirs(outPath, exist_ok=True)\n",
        "            #     outFileName = os.path.join(outPath, str(datetime.datetime.now()).replace(\" \",\"_\") + \".jpg\")\n",
        "\n",
        "            #     # resizedImg = cv2.resize(cvImage, (800, 600), interpolation = cv2.INTER_AREA)\n",
        "            #     # retval = cv2.imwrite(outFileName, resizedImg)\n",
        "            #     retval = cv2.imwrite(outFileName, cvImage)\n",
        "\n",
        "            # # Log only in debug mode\n",
        "            # logger.info(\"[AI EXT] Sending response. Status: {0} - Count: {1}\".format(statusCode, objectCount))\n",
        "\n",
        "        logger.info(\"[AI EXT] Sending response.\")\n",
        "        return Response(result, status= 200, mimetype ='application/json')\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.info(\"[AI EXT] Exception (scoreRRS): {0}\".format(str(e)))\n",
        "        return Response(response='Exception occured while processing the image.', status=500)   \n",
        "        #return Response(json.dumps({}), status= 200, mimetype ='application/json')   \n",
        "    \n",
        "@app.route(\"/\")\n",
        "def healthy():\n",
        "    return \"Healthy\"\n",
        "\n",
        "# Version\n",
        "@app.route('/version', methods = ['GET'])\n",
        "def version_request():\n",
        "    global analyticsAPI\n",
        "    return analyticsAPI.version()\n",
        "\n",
        "# About\n",
        "@app.route('/about', methods = ['GET'])\n",
        "def about_request():\n",
        "    global analyticsAPI\n",
        "    return analyticsAPI.about()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    while not analyticsAPI.initialized:\n",
        "        logger.info(\"[AI EXT] Waiting AI module to be initialized. (app.py)\")\n",
        "        time.sleep(1)\n",
        "        \n",
        "    app.run(host='0.0.0.0', port=5444)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the cell above, 5444 is the internal port of the webserver app that listens the requests. Next, we will map it to different ports to expose it externally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/wsgi.py\n",
        "from app import app as application\n",
        "\n",
        "def create():\n",
        "    application.run(host='127.0.0.1', port=5444)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(os.path.join(isSolutionPath, \"nginx\"), exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The exposed port of the web app is now 5001, while the internal one is still 5444."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/nginx/app\n",
        "server {\n",
        "    listen 5001;\n",
        "    server_name _;\n",
        " \n",
        "    location / {\n",
        "    include proxy_params;\n",
        "    proxy_pass http://127.0.0.1:5444;\n",
        "    proxy_connect_timeout 5000s;\n",
        "    proxy_read_timeout 5000s;\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/gunicorn_logging.conf\n",
        "\n",
        "[loggers]\n",
        "keys=root, gunicorn.error\n",
        "\n",
        "[handlers]\n",
        "keys=console\n",
        "\n",
        "[formatters]\n",
        "keys=json\n",
        "\n",
        "[logger_root]\n",
        "level=INFO\n",
        "handlers=console\n",
        "\n",
        "[logger_gunicorn.error]\n",
        "level=ERROR\n",
        "handlers=console\n",
        "propagate=0\n",
        "qualname=gunicorn.error\n",
        "\n",
        "[handler_console]\n",
        "class=StreamHandler\n",
        "formatter=json\n",
        "args=(sys.stdout, )\n",
        "\n",
        "[formatter_json]\n",
        "class=jsonlogging.JSONFormatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/kill_supervisor.py\n",
        "import sys\n",
        "import os\n",
        "import signal\n",
        "\n",
        "def write_stdout(s):\n",
        "    sys.stdout.write(s)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "# this function is modified from the code and knowledge found here: http://supervisord.org/events.html#example-event-listener-implementation\n",
        "def main():\n",
        "    while 1:\n",
        "        write_stdout('[AI EXT] READY\\n')\n",
        "        # wait for the event on stdin that supervisord will send\n",
        "        line = sys.stdin.readline()\n",
        "        write_stdout('[AI EXT] Terminating supervisor with this event: ' + line);\n",
        "        try:\n",
        "            # supervisord writes its pid to its file from which we read it here, see supervisord.conf\n",
        "            pidfile = open('/tmp/supervisord.pid','r')\n",
        "            pid = int(pidfile.readline());\n",
        "            os.kill(pid, signal.SIGQUIT)\n",
        "        except Exception as e:\n",
        "            write_stdout('[AI EXT] Could not terminate supervisor: ' + e.strerror + '\\n')\n",
        "            write_stdout('[AI EXT] RESULT 2\\nOK')\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(os.path.join(isSolutionPath, \"etc\"), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/etc/supervisord.conf \n",
        "[supervisord]\n",
        "logfile=/tmp/supervisord.log ; (main log file;default $CWD/supervisord.log)\n",
        "logfile_maxbytes=50MB        ; (max main logfile bytes b4 rotation;default 50MB)\n",
        "logfile_backups=10           ; (num of main logfile rotation backups;default 10)\n",
        "loglevel=info                ; (log level;default info; others: debug,warn,trace)\n",
        "pidfile=/tmp/supervisord.pid ; (supervisord pidfile;default supervisord.pid)\n",
        "nodaemon=true                ; (start in foreground if true;default false)\n",
        "minfds=1024                  ; (min. avail startup file descriptors;default 1024)\n",
        "minprocs=200                 ; (min. avail process descriptors;default 200)\n",
        "\n",
        "# TODO DO WE NEED THIS?\n",
        "environment=LD_LIBRARY_PATH=%(ENV_LD_LIBRARY_PATH)s,INTEL_CVSDK_DIR=%(ENV_INTEL_CVSDK_DIR)s,OpenCV_DIR=%(ENV_OpenCV_DIR)s,InferenceEngine_DIR=%(ENV_InferenceEngine_DIR)s,PYTHONPATH=%(ENV_PYTHONPATH)s,INTEL_OPENVINO_DIR=%(ENV_INTEL_OPENVINO_DIR)s,PATH=%(ENV_PATH)s,HDDL_INSTALL_DIR=%(ENV_HDDL_INSTALL_DIR)s,INTEL_OPENVINO_DIR=%(ENV_INTEL_OPENVINO_DIR)s,PATH=%(ENV_PATH)s\n",
        "\n",
        "[program:gunicorn]\n",
        "command=bash -c \"gunicorn --workers 1 -m 007 --timeout 100000 --capture-output --error-logfile - --log-level debug --log-config gunicorn_logging.conf \\\"wsgi:create()\\\"\"\n",
        "directory=/isserver\n",
        "redirect_stderr=true\n",
        "stdout_logfile =/dev/stdout\n",
        "stdout_logfile_maxbytes=0\n",
        "startretries=2\n",
        "startsecs=20\n",
        "\n",
        "[program:nginx]\n",
        "command=/usr/sbin/nginx -g \"daemon off;\"\n",
        "startretries=2\n",
        "startsecs=5\n",
        "priority=3\n",
        "\n",
        "[eventlistener:program_exit]\n",
        "command=python kill_supervisor.py\n",
        "directory=/isserver\n",
        "events=PROCESS_STATE_FATAL\n",
        "priority=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO Do we need this?\n",
        "\n",
        "%%writefile $isSolutionPath/requirements.txt\n",
        "pillow<7.0.0\n",
        "click==6.7\n",
        "configparser==3.5.0\n",
        "Flask==0.12.2\n",
        "gunicorn==19.6.0\n",
        "json-logging-py==0.2\n",
        "MarkupSafe==1.0\n",
        "olefile==0.44\n",
        "requests==2.12.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Docker File to Containerize the ML Solution and Web App Server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> <span style=\"color:red; font-weight: bold; font-size:1.1em;\"> [!IMPORTANT] </span>  \n",
        "\n",
        "> The OpenVINO™ Toolkit is a licenced software. To ensure that you are using the latest version of the OpenVINO™ Toolkit, follow these instructions to obtain a licensed download link:  \n",
        "\n",
        "> 1) Go to the [Intel donwload link](https://software.intel.com/en-us/openvino-toolkit/choose-download/free-download-linux) for the OpenVINO™ Toolkit\n",
        "\n",
        "> 2) Click on the \"Register & Download\" button  \n",
        "\n",
        "> <img src=\"../../../../../images/_openvino_img_03_001.jpg\" width=400 alt=\"> Figure: Register & Download.\"/>  \n",
        "\n",
        "> 3) Fill in the form and click submit \n",
        "\n",
        "> <img src=\"../../../../../images/_openvino_img_03_002.jpg\" width=400 alt=\"> Figure: Fill the form.\"/>  \n",
        "\n",
        "> 4) Over the \"Full Package\" link, right click and get the link which should look like something:  \n",
        "    http://registrationcenter-download.intel.com/akdlm/irc_nas/<SOMECODE\\>/l_openvino_toolkit_p_2020.3.194.tgz  \n",
        "    \n",
        "> <img src=\"../../../../../images/_openvino_img_03_003.jpg\" width=400 alt=\"> Figure: Download link.\"/>  \n",
        "\n",
        "> 5) In the below cell, set the value of variable \"openVinoToolkitDownloadLink\" to the download link you have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# As described above, set the value of variable \"openVinoToolkitDownloadLink\" to the download link you have (below is sample URI, just remove it and use your own address)\n",
        "\n",
        "# openVinoToolkitDownloadLink = \"http://registrationcenter-download.intel.com/akdlm/irc_nas/<SOMECODE>/l_openvino_toolkit_p_2020.3.194.tgz\"\n",
        "openVinoToolkitDownloadLink = \"<YOUR_LINK>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/Dockerfile\n",
        "\n",
        "FROM ubuntu:18.04\n",
        "\n",
        "USER root\n",
        "\n",
        "ARG WORK_DIR=/isserver\n",
        "ENV WORK_DIR ${WORK_DIR}\n",
        "ENV PATH /opt/miniconda/bin:${PATH}\n",
        "\n",
        "RUN mkdir -p ${WORK_DIR}\n",
        "\n",
        "WORKDIR ${WORK_DIR}\n",
        "\n",
        "#\n",
        "# Install base\n",
        "#\n",
        "RUN apt-get update &&\\\n",
        "    apt-get install -y --no-install-recommends \\\n",
        "        # Essentials\n",
        "        wget \\\n",
        "        locales \\\n",
        "        # Python environment\n",
        "        python3 \\\n",
        "        python3-setuptools &&\\\n",
        "    #\n",
        "    # Dependencies: conda\n",
        "    wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.5.11-Linux-x86_64.sh -O ${WORK_DIR}/miniconda.sh --no-check-certificate &&\\ \n",
        "    /bin/bash ${WORK_DIR}/miniconda.sh -b -p /opt/miniconda &&\\\n",
        "    #\n",
        "    # Cleaning\n",
        "    /opt/miniconda/bin/conda clean -ya &&\\\n",
        "    rm -rf /opt/miniconda/pkgs &&\\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "#\n",
        "# Install OpenVINO™\n",
        "#\n",
        "#COPY l_openvino_toolkit_p_2020.1.023.tgz ${WORK_DIR}\n",
        "RUN apt-get update &&\\\n",
        "    apt-get install -y --no-install-recommends \\\n",
        "        # Essentials\n",
        "        cpio \\\n",
        "        udev \\\n",
        "        unzip \\\n",
        "        autoconf \\\n",
        "        automake \\\n",
        "        libtool\n",
        "\n",
        "RUN wget --quiet IS_OPENVINO_TOOLKIT_DOWNLOAD_LINK -O ${WORK_DIR}/l_openvino_toolkit_p_2020.1.023.tgz &&\\\n",
        "    pattern=\"COMPONENTS=DEFAULTS\" &&\\\n",
        "    replacement=\"COMPONENTS=intel-openvino-ie-sdk-ubuntu-bionic__x86_64;intel-openvino-ie-rt-cpu-ubuntu-bionic__x86_64;intel-openvino-ie-rt-vpu-ubuntu-bionic__x86_64;intel-openvino-opencv-lib-ubuntu-bionic__x86_64\" &&\\\n",
        "    tar -xzf l_openvino_toolkit*.tgz &&\\\n",
        "    cd l_openvino_toolkit* &&\\\n",
        "    sed -i \"s/$pattern/$replacement/\" silent.cfg &&\\\n",
        "    sed -i \"s/decline/accept/g\" silent.cfg &&\\\n",
        "    /bin/bash ./install.sh -s silent.cfg &&\\\n",
        "    cd - &&\\\n",
        "    cd /opt/intel/openvino/install_dependencies &&\\\n",
        "    /bin/bash ./install_openvino_dependencies.sh &&\\\n",
        "    # setup environment variables\n",
        "    echo \"source /opt/intel/openvino/bin/setupvars.sh\" >> /root/.bashrc &&\\\n",
        "    #\n",
        "    # Cleaning\n",
        "    cd ${WORK_DIR} &&\\\n",
        "    rm -rf * &&\\\n",
        "    /opt/miniconda/bin/conda clean -ya &&\\\n",
        "    rm -rf /opt/miniconda/pkgs &&\\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "#\n",
        "# Set environment variables as in ${INTEL_OPENVINO_DIR}/bin/setupvars.sh\n",
        "ENV INTEL_OPENVINO_DIR /opt/intel/openvino\n",
        "ENV LD_LIBRARY_PATH ${INTEL_OPENVINO_DIR}/opencv/lib:${INTEL_OPENVINO_DIR}/deployment_tools/ngraph/lib:/opt/intel/opencl:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/hddl/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/gna/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/mkltiny_lnx/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/tbb/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64\n",
        "ENV INTEL_CVSDK_DIR ${INTEL_OPENVINO_DIR}\n",
        "ENV OpenCV_DIR ${INTEL_OPENVINO_DIR}/opencv/cmake\n",
        "ENV InferenceEngine_DIR ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/share\n",
        "ENV PYTHONPATH ${INTEL_OPENVINO_DIR}/python/python3.7:${INTEL_OPENVINO_DIR}/python/python3:${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/accuracy_checker:${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer\n",
        "ENV PATH ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:${PATH}\n",
        "ENV HDDL_INSTALL_DIR ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/hddl\n",
        "\n",
        "#\n",
        "# Exclude UDEV by rebuilding libusb without UDEV support\n",
        "RUN cp ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/97-myriad-usbboot.rules /etc/udev/rules.d/ &&\\\n",
        "    ldconfig &&\\\n",
        "    cd /opt && wget --quiet --no-check-certificate http://github.com/libusb/libusb/archive/v1.0.22.zip -O /opt/v1.0.22.zip &&\\\n",
        "    unzip v1.0.22.zip && cd libusb-1.0.22 &&\\\n",
        "    ./bootstrap.sh &&\\\n",
        "    ./configure --disable-udev --enable-shared &&\\\n",
        "    make -j4\n",
        "\n",
        "RUN apt-get update &&\\\n",
        "    apt-get install -y --no-install-recommends libusb-1.0-0-dev &&\\\n",
        "    cd /opt &&\\\n",
        "    rm -rf /var/lib/apt/lists/* &&\\\n",
        "    cd /opt/libusb-1.0.22/libusb &&\\\n",
        "    /bin/mkdir -p '/usr/local/lib' &&\\\n",
        "    /bin/bash ../libtool --mode=install /usr/bin/install -c libusb-1.0.la '/usr/local/lib' &&\\\n",
        "    /bin/mkdir -p '/usr/local/include/libusb-1.0' &&\\\n",
        "    /usr/bin/install -c -m 644 libusb.h '/usr/local/include/libusb-1.0' &&\\\n",
        "    /bin/mkdir -p '/usr/local/lib/pkgconfig' &&\\\n",
        "    cd /opt/libusb-1.0.22/ &&\\\n",
        "    /usr/bin/install -c -m 644 libusb-1.0.pc '/usr/local/lib/pkgconfig' &&\\\n",
        "    ldconfig\n",
        "\n",
        "#\n",
        "# Install ML solution\n",
        "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
        "    nginx \\\n",
        "    supervisor &&\\\n",
        "    pip install \\\n",
        "        numpy \\\n",
        "        azure-iot-device\n",
        "        \n",
        "ADD . ${WORK_DIR}\n",
        "ADD etc /etc\n",
        "\n",
        "RUN rm -rf /var/lib/apt/lists/* &&\\\n",
        "    rm /etc/nginx/sites-enabled/default &&\\\n",
        "    cp ${WORK_DIR}/nginx/app /etc/nginx/sites-available/ &&\\\n",
        "    ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/ &&\\\n",
        "    pip install -r ${WORK_DIR}/requirements.txt &&\\\n",
        "    /opt/miniconda/bin/conda clean -ya &&\\\n",
        "    rm -rf /opt/miniconda/pkgs &&\\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "EXPOSE 5001\n",
        "CMD [\"supervisord\", \"-c\", \"/isserver/etc/supervisord.conf\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update Docker file with custom environment variable: IoT Edge device's connection string\n",
        "filePath = isSolutionPath+\"/Dockerfile\"\n",
        "file = open(filePath)\n",
        "dockerFileTemplate = file.read()\n",
        "dockerFileTemplate = dockerFileTemplate.replace(\"IS_OPENVINO_TOOLKIT_DOWNLOAD_LINK\", \"\\\"\"+openVinoToolkitDownloadLink+\"\\\"\")\n",
        "\n",
        "with open(filePath, 'wt', encoding='utf-8') as outputFile:\n",
        "    outputFile.write(dockerFileTemplate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Local Docker Image\n",
        "Finally, we will create a Docker image locally. We will later host the image in a container registry like Docker Hub, Azure Container Registry, or a local registry.\n",
        "\n",
        "To run the following code snippet, you must have the pre-requisities mentioned in [the requirements page](/yolov3-ngpu-onnx/01_requirements.md). Most notably, we are running the `docker` command without `sudo`.\n",
        "\n",
        "> <span>[!WARNING]</span>\n",
        "> Please ensure that Docker is running before executing the cell below. Execution of the cell below may take several minutes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!docker build -t $containerImageName --file ./$isSolutionPath/Dockerfile ./$isSolutionPath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "If all the code cells above have successfully finished running, return to the Readme page to continue.   "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}