{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "# Create a Local Docker Image\n",
        "In this section, we will create an IoT Edge module, a Docker container image with an HTTP web server that has a scoring REST endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../../common')\n",
        "from env_variables import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Web Application & Inference Server for Our ML Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. To change the inference model that will be used in this sample, change the variable `IS_MODEL_NAME` based on your preferred model. Note that the model must be downloaded, as instructed in the [previous section](create_openvino_inference_engine.ipynb). The default model is         \"person-vehicle-bike-detection-crossroad-1016\".\n",
        "\n",
        "2. The variable assignment of `IS_TARGET_DEVICE` indicates what type of hardware acceleration you would like to use on your IoT Edge device. You can choose from the following choices:\n",
        "    * \"CPU\" for Intel® CPU acceleration  \n",
        "    * \"MYRIAD\" for Intel® VPU acceleration\n",
        "    * \"GPU\" for Intel® GPU acceleration\n",
        "    * \"FPGA\" for Intel® FPGA acceleration\n",
        "\n",
        "    Change the variable `IS_TARGET_DEVICE` as needed; the default is \"CPU\". \n",
        "\n",
        "3. The variable assignment of `IS_MODEL_PRECISION` indicates what type of model precision you would like to use. The default is \"FP32\" for this sample. However, please [check the Intel documentation](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html) for which output precision is supported for your desired hardware acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/app.py\n",
        "import threading\n",
        "import cv2\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import linecache\n",
        "from score import MLModel, PrintGetExceptionDetails\n",
        "from flask import Flask, request, Response\n",
        "\n",
        "# Initial settings of AI model\n",
        "IS_MODEL_NAME = \"person-vehicle-bike-detection-crossroad-1016\" # see MLModel class for full list of models and other possibilities\n",
        "IS_TARGET_DEVICE = \"CPU\"\n",
        "IS_MODEL_PRECISION = \"FP32\"\n",
        "\n",
        "app = Flask(__name__)\n",
        "inferenceEngine = MLModel(    modelName=IS_MODEL_NAME, \n",
        "                                modelPrecision=IS_MODEL_PRECISION, \n",
        "                                targetDev=IS_TARGET_DEVICE, \n",
        "                            )\n",
        "\n",
        "@app.route(\"/score\", methods = ['POST'])\n",
        "def scoreRRS():\n",
        "    global inferenceEngine\n",
        "\n",
        "    try:\n",
        "        # get request as byte stream\n",
        "        reqBody = request.get_data(False)\n",
        "\n",
        "        # convert from byte stream\n",
        "        inMemFile = io.BytesIO(reqBody)\n",
        "\n",
        "        # load a sample image\n",
        "        inMemFile.seek(0)\n",
        "        fileBytes = np.asarray(bytearray(inMemFile.read()), dtype=np.uint8)\n",
        "        cvImage = cv2.imdecode(fileBytes, cv2.IMREAD_COLOR)\n",
        "\n",
        "        # call scoring function\n",
        "        detectedObjects = inferenceEngine.score(cvImage)            \n",
        "\n",
        "        if len(detectedObjects) > 0:\n",
        "            respBody = {                    \n",
        "                        \"inferences\" : detectedObjects\n",
        "                    }\n",
        "\n",
        "            respBody = json.dumps(respBody)\n",
        "            \n",
        "            logging.info(\"[AI EXT] Sending response.\")\n",
        "            return Response(respBody, status= 200, mimetype ='application/json')\n",
        "        else:\n",
        "            logging.info(\"[AI EXT] Sending empty response.\")\n",
        "            return Response(status= 204)\n",
        "\n",
        "    except:\n",
        "        PrintGetExceptionDetails()\n",
        "        return Response(response='Exception occured while processing the image.', status=500)\n",
        "    \n",
        "@app.route(\"/\")\n",
        "def healthy():\n",
        "    return \"Healthy\"\n",
        "\n",
        "# About\n",
        "@app.route('/about', methods = ['GET'])\n",
        "def about_request():\n",
        "    global inferenceEngine\n",
        "    return inferenceEngine.about()\n",
        "\n",
        "if __name__ == \"__main__\":      \n",
        "    app.run(host='127.0.0.1', port=5444)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the cell above, 5444 is the internal port of the webserver app that listens the requests. Next, we will map it to different ports to expose it externally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/wsgi.py\n",
        "from app import app as application\n",
        "\n",
        "def create():\n",
        "    application.run(host='127.0.0.1', port=5444)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(os.path.join(isSolutionPath, \"nginx\"), exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The exposed port of the web app is now 5001, while the internal one is still 5444."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/nginx/app\n",
        "server {\n",
        "    listen 5001;\n",
        "    server_name _;\n",
        " \n",
        "    location / {\n",
        "    include proxy_params;\n",
        "    proxy_pass http://127.0.0.1:5444;\n",
        "    proxy_connect_timeout 5000s;\n",
        "    proxy_read_timeout 5000s;\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/gunicorn_logging.conf\n",
        "\n",
        "[loggers]\n",
        "keys=root, gunicorn.error\n",
        "\n",
        "[handlers]\n",
        "keys=console\n",
        "\n",
        "[formatters]\n",
        "keys=json\n",
        "\n",
        "[logger_root]\n",
        "level=INFO\n",
        "handlers=console\n",
        "\n",
        "[logger_gunicorn.error]\n",
        "level=ERROR\n",
        "handlers=console\n",
        "propagate=0\n",
        "qualname=gunicorn.error\n",
        "\n",
        "[handler_console]\n",
        "class=StreamHandler\n",
        "formatter=json\n",
        "args=(sys.stdout, )\n",
        "\n",
        "[formatter_json]\n",
        "class=jsonlogging.JSONFormatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/kill_supervisor.py\n",
        "import sys\n",
        "import os\n",
        "import signal\n",
        "\n",
        "def write_stdout(s):\n",
        "    sys.stdout.write(s)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "# this function is modified from the code and knowledge found here: http://supervisord.org/events.html#example-event-listener-implementation\n",
        "def main():\n",
        "    while 1:\n",
        "        write_stdout('[AI EXT] READY\\n')\n",
        "        # wait for the event on stdin that supervisord will send\n",
        "        line = sys.stdin.readline()\n",
        "        write_stdout('[AI EXT] Terminating supervisor with this event: ' + line);\n",
        "        try:\n",
        "            # supervisord writes its pid to its file from which we read it here, see supervisord.conf\n",
        "            pidfile = open('/tmp/supervisord.pid','r')\n",
        "            pid = int(pidfile.readline());\n",
        "            os.kill(pid, signal.SIGQUIT)\n",
        "        except Exception as e:\n",
        "            write_stdout('[AI EXT] Could not terminate supervisor: ' + e.strerror + '\\n')\n",
        "            write_stdout('[AI EXT] RESULT 2\\nOK')\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(os.path.join(isSolutionPath, \"etc\"), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/etc/supervisord.conf \n",
        "[supervisord]\n",
        "logfile=/tmp/supervisord.log ; (main log file;default $CWD/supervisord.log)\n",
        "logfile_maxbytes=50MB        ; (max main logfile bytes b4 rotation;default 50MB)\n",
        "logfile_backups=10           ; (num of main logfile rotation backups;default 10)\n",
        "loglevel=info                ; (log level;default info; others: debug,warn,trace)\n",
        "pidfile=/tmp/supervisord.pid ; (supervisord pidfile;default supervisord.pid)\n",
        "nodaemon=true                ; (start in foreground if true;default false)\n",
        "minfds=1024                  ; (min. avail startup file descriptors;default 1024)\n",
        "minprocs=200                 ; (min. avail process descriptors;default 200)\n",
        "\n",
        "environment=LD_LIBRARY_PATH=%(ENV_LD_LIBRARY_PATH)s,INTEL_CVSDK_DIR=%(ENV_INTEL_CVSDK_DIR)s,OpenCV_DIR=%(ENV_OpenCV_DIR)s,InferenceEngine_DIR=%(ENV_InferenceEngine_DIR)s,PYTHONPATH=%(ENV_PYTHONPATH)s,INTEL_OPENVINO_DIR=%(ENV_INTEL_OPENVINO_DIR)s,PATH=%(ENV_PATH)s,HDDL_INSTALL_DIR=%(ENV_HDDL_INSTALL_DIR)s,INTEL_OPENVINO_DIR=%(ENV_INTEL_OPENVINO_DIR)s,PATH=%(ENV_PATH)s\n",
        "\n",
        "[program:gunicorn]\n",
        "command=bash -c \"gunicorn --workers 1 -m 007 --timeout 100000 --capture-output --error-logfile - --log-level debug --log-config gunicorn_logging.conf \\\"wsgi:create()\\\"\"\n",
        "directory=/isserver\n",
        "redirect_stderr=true\n",
        "stdout_logfile =/dev/stdout\n",
        "stdout_logfile_maxbytes=0\n",
        "startretries=2\n",
        "startsecs=20\n",
        "\n",
        "[program:nginx]\n",
        "command=/usr/sbin/nginx -g \"daemon off;\"\n",
        "startretries=2\n",
        "startsecs=5\n",
        "priority=3\n",
        "\n",
        "[eventlistener:program_exit]\n",
        "command=python kill_supervisor.py\n",
        "directory=/isserver\n",
        "events=PROCESS_STATE_FATAL\n",
        "priority=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/requirements.txt\n",
        "pillow<7.0.0\n",
        "click==6.7\n",
        "configparser==3.5.0\n",
        "Flask==0.12.2\n",
        "gunicorn==19.6.0\n",
        "json-logging-py==0.2\n",
        "MarkupSafe==1.0\n",
        "olefile==0.44\n",
        "requests==2.12.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Docker File to Containerize the ML Solution and Web App Server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> <span style=\"color:red; font-weight: bold; font-size:1.1em;\"> [!IMPORTANT] </span>  \n",
        "\n",
        "> The OpenVINO™ Toolkit is a licenced software. To ensure that you are using the latest version of the OpenVINO™ Toolkit, follow these instructions to obtain a licensed download link:  \n",
        "\n",
        "> 1) Go to the [Intel donwload link](https://software.intel.com/en-us/openvino-toolkit/choose-download/free-download-linux) for the OpenVINO™ Toolkit\n",
        "\n",
        "> 2) Click on the \"Register & Download\" button  \n",
        "\n",
        "> <img src=\"../../../../images/_openvino_img_03_001.jpg\" width=400 alt=\"> Figure: Register & Download.\"/>  \n",
        "\n",
        "> 3) Fill in the form and click submit \n",
        "\n",
        "> <img src=\"../../../../images/_openvino_img_03_002.jpg\" width=400 alt=\"> Figure: Fill the form.\"/>  \n",
        "\n",
        "> 4) Over the \"Full Package\" link, right click and get the link which should look like something:  \n",
        "    http://registrationcenter-download.intel.com/akdlm/irc_nas/<SOMECODE\\>/l_openvino_toolkit_p_2020.3.194.tgz  \n",
        "    \n",
        "> <img src=\"../../../../images/_openvino_img_03_003.jpg\" width=400 alt=\"> Figure: Download link.\"/>  \n",
        "\n",
        "> 5) In the below cell, set the value of variable \"openVinoToolkitDownloadLink\" to the download link you have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# As described above, set the value of variable \"openVinoToolkitDownloadLink\" to the download link you have (below is sample URI, just remove it and use your own address)\n",
        "\n",
        "# openVinoToolkitDownloadLink = \"http://registrationcenter-download.intel.com/akdlm/irc_nas/<SOMECODE>/l_openvino_toolkit_p_2020.3.194.tgz\"\n",
        "openVinoToolkitDownloadLink = \"<YOUR_LINK>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $isSolutionPath/Dockerfile\n",
        "\n",
        "FROM ubuntu:18.04\n",
        "\n",
        "USER root\n",
        "\n",
        "ARG WORK_DIR=/isserver\n",
        "ENV WORK_DIR ${WORK_DIR}\n",
        "ENV PATH /opt/miniconda/bin:${PATH}\n",
        "\n",
        "RUN mkdir -p ${WORK_DIR}\n",
        "\n",
        "WORKDIR ${WORK_DIR}\n",
        "\n",
        "#\n",
        "# Install base\n",
        "#\n",
        "RUN apt-get update &&\\\n",
        "    apt-get install -y --no-install-recommends \\\n",
        "        # Essentials\n",
        "        wget \\\n",
        "        locales \\\n",
        "        # Python environment\n",
        "        python3 \\\n",
        "        python3-setuptools &&\\\n",
        "    #\n",
        "    # Dependencies: conda\n",
        "    wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.5.11-Linux-x86_64.sh -O ${WORK_DIR}/miniconda.sh --no-check-certificate &&\\ \n",
        "    /bin/bash ${WORK_DIR}/miniconda.sh -b -p /opt/miniconda &&\\\n",
        "    #\n",
        "    # Cleaning\n",
        "    /opt/miniconda/bin/conda clean -ya &&\\\n",
        "    rm -rf /opt/miniconda/pkgs &&\\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "#\n",
        "# Install OpenVINO™\n",
        "#\n",
        "#COPY l_openvino_toolkit_p_2020.1.023.tgz ${WORK_DIR}\n",
        "RUN apt-get update &&\\\n",
        "    apt-get install -y --no-install-recommends \\\n",
        "        # Essentials\n",
        "        cpio \\\n",
        "        udev \\\n",
        "        unzip \\\n",
        "        autoconf \\\n",
        "        automake \\\n",
        "        libtool\n",
        "\n",
        "RUN wget --quiet IS_OPENVINO_TOOLKIT_DOWNLOAD_LINK -O ${WORK_DIR}/l_openvino_toolkit_p_2020.1.023.tgz &&\\\n",
        "    pattern=\"COMPONENTS=DEFAULTS\" &&\\\n",
        "    replacement=\"COMPONENTS=intel-openvino-ie-sdk-ubuntu-bionic__x86_64;intel-openvino-ie-rt-cpu-ubuntu-bionic__x86_64;intel-openvino-ie-rt-vpu-ubuntu-bionic__x86_64;intel-openvino-opencv-lib-ubuntu-bionic__x86_64\" &&\\\n",
        "    tar -xzf l_openvino_toolkit*.tgz &&\\\n",
        "    cd l_openvino_toolkit* &&\\\n",
        "    sed -i \"s/$pattern/$replacement/\" silent.cfg &&\\\n",
        "    sed -i \"s/decline/accept/g\" silent.cfg &&\\\n",
        "    /bin/bash ./install.sh -s silent.cfg &&\\\n",
        "    cd - &&\\\n",
        "    cd /opt/intel/openvino/install_dependencies &&\\\n",
        "    /bin/bash ./install_openvino_dependencies.sh &&\\\n",
        "    # setup environment variables\n",
        "    echo \"source /opt/intel/openvino/bin/setupvars.sh\" >> /root/.bashrc &&\\\n",
        "    #\n",
        "    # Cleaning\n",
        "    cd ${WORK_DIR} &&\\\n",
        "    rm -rf * &&\\\n",
        "    /opt/miniconda/bin/conda clean -ya &&\\\n",
        "    rm -rf /opt/miniconda/pkgs &&\\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "#\n",
        "# Set environment variables as in ${INTEL_OPENVINO_DIR}/bin/setupvars.sh\n",
        "ENV INTEL_OPENVINO_DIR /opt/intel/openvino\n",
        "ENV LD_LIBRARY_PATH ${INTEL_OPENVINO_DIR}/opencv/lib:${INTEL_OPENVINO_DIR}/deployment_tools/ngraph/lib:/opt/intel/opencl:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/hddl/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/gna/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/mkltiny_lnx/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/tbb/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64\n",
        "ENV INTEL_CVSDK_DIR ${INTEL_OPENVINO_DIR}\n",
        "ENV OpenCV_DIR ${INTEL_OPENVINO_DIR}/opencv/cmake\n",
        "ENV InferenceEngine_DIR ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/share\n",
        "ENV PYTHONPATH ${INTEL_OPENVINO_DIR}/python/python3.7:${INTEL_OPENVINO_DIR}/python/python3:${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/accuracy_checker:${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer\n",
        "ENV PATH ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:${PATH}\n",
        "ENV HDDL_INSTALL_DIR ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/hddl\n",
        "\n",
        "#\n",
        "# Exclude UDEV by rebuilding libusb without UDEV support\n",
        "RUN cp ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/97-myriad-usbboot.rules /etc/udev/rules.d/ &&\\\n",
        "    ldconfig &&\\\n",
        "    cd /opt && wget --quiet --no-check-certificate http://github.com/libusb/libusb/archive/v1.0.22.zip -O /opt/v1.0.22.zip &&\\\n",
        "    unzip v1.0.22.zip && cd libusb-1.0.22 &&\\\n",
        "    ./bootstrap.sh &&\\\n",
        "    ./configure --disable-udev --enable-shared &&\\\n",
        "    make -j4\n",
        "\n",
        "RUN apt-get update &&\\\n",
        "    apt-get install -y --no-install-recommends libusb-1.0-0-dev &&\\\n",
        "    cd /opt &&\\\n",
        "    rm -rf /var/lib/apt/lists/* &&\\\n",
        "    cd /opt/libusb-1.0.22/libusb &&\\\n",
        "    /bin/mkdir -p '/usr/local/lib' &&\\\n",
        "    /bin/bash ../libtool --mode=install /usr/bin/install -c libusb-1.0.la '/usr/local/lib' &&\\\n",
        "    /bin/mkdir -p '/usr/local/include/libusb-1.0' &&\\\n",
        "    /usr/bin/install -c -m 644 libusb.h '/usr/local/include/libusb-1.0' &&\\\n",
        "    /bin/mkdir -p '/usr/local/lib/pkgconfig' &&\\\n",
        "    cd /opt/libusb-1.0.22/ &&\\\n",
        "    /usr/bin/install -c -m 644 libusb-1.0.pc '/usr/local/lib/pkgconfig' &&\\\n",
        "    ldconfig\n",
        "\n",
        "#\n",
        "# Install ML solution\n",
        "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
        "    nginx \\\n",
        "    supervisor &&\\\n",
        "    pip install \\\n",
        "        numpy \\\n",
        "        azure-iot-device\n",
        "        \n",
        "ADD . ${WORK_DIR}\n",
        "ADD etc /etc\n",
        "\n",
        "RUN rm -rf /var/lib/apt/lists/* &&\\\n",
        "    rm /etc/nginx/sites-enabled/default &&\\\n",
        "    cp ${WORK_DIR}/nginx/app /etc/nginx/sites-available/ &&\\\n",
        "    ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/ &&\\\n",
        "    pip install -r ${WORK_DIR}/requirements.txt &&\\\n",
        "    /opt/miniconda/bin/conda clean -ya &&\\\n",
        "    rm -rf /opt/miniconda/pkgs &&\\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "EXPOSE 5001\n",
        "CMD [\"supervisord\", \"-c\", \"/isserver/etc/supervisord.conf\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update Docker file with custom environment variable: IoT Edge device's connection string\n",
        "filePath = isSolutionPath+\"/Dockerfile\"\n",
        "file = open(filePath)\n",
        "dockerFileTemplate = file.read()\n",
        "dockerFileTemplate = dockerFileTemplate.replace(\"IS_OPENVINO_TOOLKIT_DOWNLOAD_LINK\", \"\\\"\"+openVinoToolkitDownloadLink+\"\\\"\")\n",
        "\n",
        "with open(filePath, 'wt', encoding='utf-8') as outputFile:\n",
        "    outputFile.write(dockerFileTemplate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Local Docker Image\n",
        "Finally, we will create a Docker image locally. We will later host the image in a container registry like Docker Hub, Azure Container Registry, or a local registry.\n",
        "\n",
        "To run the following code snippet, you must have the pre-requisities mentioned in [the requirements page](/yolov3-ngpu-onnx/01_requirements.md). Most notably, we are running the `docker` command without `sudo`.\n",
        "\n",
        "> <span>[!WARNING]</span>\n",
        "> Please ensure that Docker is running before executing the cell below. Execution of the cell below may take several minutes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!docker build -t $containerImageName --file ./$isSolutionPath/Dockerfile ./$isSolutionPath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "If all the code cells above have successfully finished running, return to the Readme page to continue.   "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}